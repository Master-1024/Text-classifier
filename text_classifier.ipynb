{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f149bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d9a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_training_data(customer):\n",
    "    \n",
    "    training_data = pd.read_parquet(\"D:/\"+customer+\"_data.parquet\")\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab8e89a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(ticket_data):  \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    ticket_data = re.sub('[^A-Za-z0-9]+', ' ', ticket_data)  \n",
    "    word_tokens = word_tokenize(ticket_data)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    out = ' '.join(filtered_sentence)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f2b0f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(training_data,description_col,target_col,customer,imbalance_percent_remove):\n",
    "    \n",
    "    training_data = training_data.dropna(subset=[description_col,target_col])\n",
    "\n",
    "    # percent of total data to be in class\n",
    "    #imbalance_percent_remove = 2\n",
    "    records_remove = (len(training_data)/100)*imbalance_percent_remove\n",
    "    print(\"Class imbalance control factor is set to \"+str(imbalance_percent_remove)+\"% of total data. Classes containing number of records below \"+ str(records_remove)+\" are removed.\")\n",
    "    \n",
    "    value_counts = training_data[target_col].value_counts()\n",
    "    value_counts = pd.DataFrame(value_counts)\n",
    "    value_counts = value_counts.reset_index()\n",
    "    value_counts.columns = ['feature','count']\n",
    "    required_feature = list(value_counts[value_counts['count']>  records_remove]['feature'])\n",
    "    training_data = training_data[training_data[target_col].isin(required_feature)]\n",
    "    training_data = training_data[[description_col,target_col]]\n",
    "    \n",
    "    training_data[description_col] = training_data.apply(lambda x: clean_text(x[description_col]), axis=1)\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(training_data[target_col])\n",
    "    filename = customer+\"_label_encoder.pkl\"\n",
    "    pickle.dump(le, open(filename, 'wb'))\n",
    "    training_data[target_col] = le.transform(training_data[target_col])\n",
    "\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cae6927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(training_data,description_col,target_col):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(training_data[description_col],training_data[target_col],test_size=0.3,random_state=42,stratify=training_data[target_col])\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da1cb83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(X_train, X_test):\n",
    "\n",
    "    embedding = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "    hub_layer = hub.KerasLayer(embedding, input_shape=[],dtype=tf.string, trainable=False,)\n",
    "    \n",
    "    data_array_X_train = X_train.to_numpy()\n",
    "    data_tensor_X_train = tf.convert_to_tensor(data_array_X_train)\n",
    "    embeddings_train= hub_layer(data_tensor_X_train)\n",
    "    print(\"train embeddings created\")\n",
    "    \n",
    "    data_array_X_test = X_test.to_numpy()\n",
    "    data_tensor_X_test = tf.convert_to_tensor(data_array_X_test)\n",
    "    embeddings_test= hub_layer(data_tensor_X_test)\n",
    "    print(\"test embeddings created\")\n",
    "    return embeddings_train, embeddings_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20ebfe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(total_class):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(120, activation = \"relu\"))\n",
    "    model.add(Dense(64, activation = \"relu\"))\n",
    "    model.add(Dense(32, activation = \"relu\"))\n",
    "    model.add(Dense(64, activation = \"relu\"))\n",
    "    model.add(Dense(120, activation = \"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(total_class, activation = \"softmax\"))\n",
    "    model.compile(Adam(lr = 0.01), \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a78583db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fiting_model(model,embeddings_train,y_train,y_test,total_class):\n",
    "    \n",
    "    y_train = to_categorical(y_train, total_class)\n",
    "    y_test = to_categorical(y_test, total_class)\n",
    "    \n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "    model.fit(embeddings_train, y_train, epochs=150, callbacks=[callback])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd7566f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,customer):\n",
    "    \n",
    "    model.save(customer+'_classifier_model')\n",
    "    return \"Model saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4118bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(customer,description_col,target_col,imbalance_percent_remove):\n",
    "\n",
    "    training_data = fetch_training_data(customer)\n",
    "    training_data = preprocess_data(training_data,description_col,target_col,customer,imbalance_percent_remove)\n",
    "    X_train, X_test, y_train, y_test = test_train_split(training_data,description_col,target_col)\n",
    "\n",
    "    embeddings_train, embeddings_test = create_embeddings(X_train, X_test)\n",
    "    total_class = len(list(y_train.unique()))\n",
    "\n",
    "    model = create_model(total_class)\n",
    "    model = fiting_model(model,embeddings_train,y_train,y_test,total_class)\n",
    "    \n",
    "    output = save_model(model,customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b11bbcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class imbalance control factor is set to 2% of total data. Classes containing number of records below 119.4 are removed.\n",
      "train embeddings created\n",
      "test embeddings created\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ketank\\Anaconda3\\envs\\env_38\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 1s 2ms/step - loss: 0.2069 - accuracy: 0.9397\n",
      "Epoch 2/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.1444 - accuracy: 0.9617\n",
      "Epoch 3/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.1442 - accuracy: 0.9624\n",
      "Epoch 4/150\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.1378 - accuracy: 0.9631\n",
      "Epoch 5/150\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.1262 - accuracy: 0.9655\n",
      "Epoch 6/150\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.1383 - accuracy: 0.9639\n",
      "Epoch 7/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.1254 - accuracy: 0.9643\n",
      "Epoch 8/150\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.1369 - accuracy: 0.9586\n",
      "Epoch 9/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.1266 - accuracy: 0.9651\n",
      "Epoch 10/150\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.1122 - accuracy: 0.9682\n",
      "Epoch 11/150\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.1022 - accuracy: 0.9677\n",
      "Epoch 12/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.1055 - accuracy: 0.9694\n",
      "Epoch 13/150\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.1063 - accuracy: 0.9679\n",
      "Epoch 14/150\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.1013 - accuracy: 0.9708\n",
      "Epoch 15/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.1008 - accuracy: 0.9715\n",
      "Epoch 16/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.1083 - accuracy: 0.9691\n",
      "Epoch 17/150\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.1859 - accuracy: 0.9569\n",
      "Epoch 18/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.1109 - accuracy: 0.9634\n",
      "Epoch 19/150\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.1108 - accuracy: 0.9660\n",
      "Epoch 20/150\n",
      "131/131 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9708\n",
      "Epoch 21/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.1078 - accuracy: 0.9689\n",
      "Epoch 22/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0890 - accuracy: 0.9742\n",
      "Epoch 23/150\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0881 - accuracy: 0.9734\n",
      "Epoch 24/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0804 - accuracy: 0.9749\n",
      "Epoch 25/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0876 - accuracy: 0.9737\n",
      "Epoch 26/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0853 - accuracy: 0.9763\n",
      "Epoch 27/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.1063 - accuracy: 0.9725\n",
      "Epoch 28/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0836 - accuracy: 0.9754\n",
      "Epoch 29/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0769 - accuracy: 0.9768\n",
      "Epoch 30/150\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0824 - accuracy: 0.9754\n",
      "Epoch 31/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0811 - accuracy: 0.9754\n",
      "Epoch 32/150\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0757 - accuracy: 0.9758\n",
      "Epoch 33/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0837 - accuracy: 0.9751\n",
      "Epoch 34/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0758 - accuracy: 0.9773\n",
      "Epoch 35/150\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0711 - accuracy: 0.9780\n",
      "Epoch 36/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0655 - accuracy: 0.9809\n",
      "Epoch 37/150\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0729 - accuracy: 0.9775\n",
      "Epoch 38/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0699 - accuracy: 0.9797\n",
      "Epoch 39/150\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.1042 - accuracy: 0.9710\n",
      "Epoch 40/150\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0846 - accuracy: 0.9782\n",
      "Epoch 41/150\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.9787\n",
      "INFO:tensorflow:Assets written to: nexops_classifier_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: nexops_classifier_model\\assets\n"
     ]
    }
   ],
   "source": [
    "train_classifier(\"dummy_customer\",\"DetailedDescription\",\"Priority\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c89653be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_group(customer,test_data):    \n",
    "    model = load_model(customer+'_classifier_model')\n",
    "\n",
    "    embedding = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\t\n",
    "    hub_layer = hub.KerasLayer(embedding, input_shape=[],dtype=tf.string, trainable=False,)\n",
    "\n",
    "    test_data = [test_data]\n",
    "    test_df = pd.DataFrame()\n",
    "    test_df[\"data\"] = test_data\n",
    "\n",
    "    data_array_X_train = test_df[\"data\"].to_numpy()\n",
    "    data_tensor_X_train = tf.convert_to_tensor(data_array_X_train)\n",
    "    embeddings_train= hub_layer(data_tensor_X_train)\n",
    "\n",
    "    y_predicted = model.predict(embeddings_train)\n",
    "    print(np.argmax(y_predicted, axis=1))\n",
    "    filename = customer+\"_label_encoder.pkl\"\n",
    "    loaded_encoder = pickle.load(open(filename, 'rb'))\n",
    "    output_class=loaded_encoder.inverse_transform(np.argmax(y_predicted, axis=1))\n",
    "    return output_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ddd0cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4 - Low'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = predict_group(\"dummy_customer\",\"Delete batch #190161072 library INFILIB\")\n",
    "output[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
